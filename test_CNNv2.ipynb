{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "mount_file_id": "1idEs4Yfe4iEXAoXqZ02VsxY3ExBLjP2V",
      "authorship_tag": "ABX9TyMrhH/UCy+XYu5u4ucndkxl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namph009/sampleapp/blob/main/test_CNNv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "zCYuJanLy18a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPEidaHiyry5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.constraints import max_norm\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "4S0YQihp1Hpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tn8E3yvLy1lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=pd.read_csv(\"/content/drive/MyDrive/Dataset/drebin-215-dataset-5560malware-9476-benign.csv\", nrows=15005, usecols=range(15, 216))"
      ],
      "metadata": {
        "id": "LKWz92egzIan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "id": "3Nb-FD9EpNGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "VlwE-GV6pU3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes,count = np.unique(df2['class'],return_counts=True)\n",
        "#Perform Label Encoding\n",
        "lbl_enc = LabelEncoder()\n",
        "print(lbl_enc.fit_transform(classes),classes)\n",
        "df2 = df2.replace(classes,lbl_enc.fit_transform(classes))\n",
        "\n",
        "#Dataset contains special characters like ''?' and 'S'. Set them to NaN and use dropna() to remove them\n",
        "df2=df2.replace('[?,S]',np.NaN,regex=True)\n",
        "print(\"Total missing values : \",sum(list(df2.isna().sum())))\n",
        "df2.dropna(inplace=True)\n",
        "for c in df2.columns:\n",
        "    df2[c] = pd.to_numeric(df2[c])\n",
        "df2"
      ],
      "metadata": {
        "id": "ko883vYzpTti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "id": "xw8yOCmms4mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vẽ vu vơ\n"
      ],
      "metadata": {
        "id": "fd5ihpBBz7jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df2.corr().abs()"
      ],
      "metadata": {
        "id": "wK9Yug-_z7D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))"
      ],
      "metadata": {
        "id": "I50nubvt4z_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(corr_matrix)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4jbf4a3t0_Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% [markdown]\n",
        "# A rather neat looking zero correlation heatmap.\n",
        "# However since the number of features are not clearly visible (due to display size) a distribution plot of the correlation matrix will \n",
        "# show how values are intertwined with each other.\n",
        "\n",
        "#%%\n",
        "dist_features = corr_matrix.values.flatten()\n",
        "\n",
        "sns.distplot(dist_features, color=\"Red\", label=\"train\")\n",
        "\n",
        "#%% [markdown]\n",
        "# A sharp spike and nothing else, this proves that the columns in the dataset are uncorrelated with each other.\n",
        "# Lets extract the significant features from the dataset, this can be achieved using random forest classifiers feature extrator routine"
      ],
      "metadata": {
        "id": "JjTpZmot1BGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(classes,count)\n",
        "plt.title(\"Class balance\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QP9ZOX3wqCSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test với model ExtraTreesClassifier"
      ],
      "metadata": {
        "id": "M3VDPPg40gOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, X_test, Y, y_test = train_test_split(df2[df2.columns[:len(df2.columns)-1]].to_numpy(),df2[df2.columns[-1]].to_numpy(),test_size = 0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "Kgrg2mCW1kwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X,Y, test_size = 0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "ATbC2HoP67-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_valid.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "OY22btm91bps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "51Wz-ntcHdjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n"
      ],
      "metadata": {
        "id": "T9-pAepFHwHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)"
      ],
      "metadata": {
        "id": "Jc-fsDXNH5xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YgwbyJF0IH_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression"
      ],
      "metadata": {
        "id": "8kXHRV-HILKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building our model with K-fold validation and GridSearch to find the best parameters\n",
        "\n",
        "# Defining all the parameters\n",
        "params = {\n",
        "    'penalty': ['l1','l2'],\n",
        "    'C': [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10]\n",
        "}\n",
        "\n",
        "# Building model\n",
        "logreg = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Parameter estimating using GridSearch\n",
        "grid = GridSearchCV(logreg, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n",
        "\n",
        "# Fitting the model\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "l5sKfNFQHevN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Best Score:', grid.best_score_)\n",
        "print('Best Params:', grid.best_params_)\n",
        "print('Best Estimator:', grid.best_estimator_)"
      ],
      "metadata": {
        "id": "g4EHd_goIByz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logreg_grid = grid.best_estimator_\n",
        "y_pred = logreg_grid.predict(X_test)"
      ],
      "metadata": {
        "id": "YrOOrYAVIt5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(confusion_matrix(y_test,y_pred), columns=[\"Predicted A\", \"Predicted T\"], index=[\"Actual A\",\"Actual T\"] )"
      ],
      "metadata": {
        "id": "acbn_cXcIwqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_grid_score = accuracy_score(y_test, y_pred)\n",
        "print('Model Accuracy:', logreg_grid_score)\n",
        "print('Classification Report:\\n', classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "DFnaTKUPIyes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Machines - XGBoost"
      ],
      "metadata": {
        "id": "jhSQ0MkHcMC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building our model with K-fold validation and GridSearch to find the best parameters\n",
        "\n",
        "# Defining all the parameters\n",
        "params = {\n",
        "    'max_depth': range (2, 10, 1),\n",
        "    'n_estimators': range(60, 220, 40),\n",
        "    'learning_rate': [0.1, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "# Building model\n",
        "xgb = XGBClassifier(objective='binary:logistic')\n",
        "\n",
        "# Parameter estimating using GridSearch\n",
        "grid = GridSearchCV(xgb, param_grid=params, scoring='accuracy', n_jobs =-1, cv=cv, verbose=1)\n",
        "\n",
        "# Fitting the model\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "4AQsOlJBcLtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST with RandomForestClassifier"
      ],
      "metadata": {
        "id": "_oSutUv212GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf=RandomForestClassifier(n_estimators=100)"
      ],
      "metadata": {
        "id": "KZiJd1Vs2F-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "kEfWmYJ7q2f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=clf.predict(X_valid)"
      ],
      "metadata": {
        "id": "_QOYNyN3rCKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred))"
      ],
      "metadata": {
        "id": "AGGBqshjrEYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(clf.estimators_)"
      ],
      "metadata": {
        "id": "aiGxg1WGrJDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## deep learning"
      ],
      "metadata": {
        "id": "2jMKwyiHq3YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Convnet(input_shape = (10,10,2),classes = 1):\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        " \n",
        "    # Stage 1 input\n",
        "    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name=\"conv1\",kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"tanh\")(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    \n",
        "    # Stage 2 hidden\n",
        "    X = Conv2D(128,kernel_size=(2,2),strides=(1,1),name=\"conv1\",kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"tanh\")(X)\n",
        "    X = Dropout(rate=0.1)(X)\n",
        "    \n",
        "    X = Conv2D(128,kernel_size=(3,3),strides=(2,2),name=\"conv1\",kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"tanh\")(X)\n",
        "    X = Dropout(rate=0.1)(X)\n",
        "    \n",
        "    X = Conv2D(256,kernel_size=(2,2),strides=(1,1),name=\"conv1\",kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"tanh\")(X)\n",
        "    X = Dropout(rate=0.1)(X)\n",
        "    \n",
        "    # Stage 3 output\n",
        "    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name=\"conv1\",kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"tanh\")(X)\n",
        " \n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='sigmoid')(X)\n",
        " \n",
        "\n",
        "    model = Model(inputs=X_input,outputs=X)\n",
        " \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "8hsl99u22rSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)"
      ],
      "metadata": {
        "id": "hSN69qGOxvvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=np.reshape(X_train,(X_train.shape[0],10,10,2))\n",
        "X_valid=np.reshape(X_valid,(X_valid.shape[0],10,10,2))\n",
        "X_test = np.reshape(X_test,(X_test.shape[0],10,10,2))"
      ],
      "metadata": {
        "id": "Oqaz-pVRrkGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "cQJ-ySQabK3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Convnet()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy'])"
      ],
      "metadata": {
        "id": "cySAyxzu23Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=50, batch_size=100, validation_data=(X_valid, y_valid),verbose = 1)"
      ],
      "metadata": {
        "id": "HsGyRR0nx11n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "4R_X8aH_2_ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bbSCdihy9CAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "# list all data in history\n",
        "print(history.history.keys())"
      ],
      "metadata": {
        "id": "1sT7c35Y9JYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z0sodzdDMZI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy,*is_anything_else_being_returned = model.evaluate(X_test, y_test, verbose=1)\n",
        "loss_v, accuracy_v, *is_anything_else_being_returned= model.evaluate(X_valid, y_valid, verbose=1)\n",
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "ru23pS3ze8b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\n",
        "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
      ],
      "metadata": {
        "id": "8peAqFArguEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}